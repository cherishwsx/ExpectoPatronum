---
description: >-
  Original Detailed Notes:
  https://github.com/hiromis/notes/blob/master/Lesson2.md
---

# Lesson 2 SGD

## Debug

### require no grad object

1. `your_var.detach().numpy()` to make it a numpy array instead of a grad True tensor
2. Also can do 

   ```python
   with torch.no_grad:
    your_code
   ```

## Tensor

1. Means array  / matrix with an regular shape. An image has 3D tensors. 

   ```python
   a = tensor(3.,2)
   ```

2. Use rank or axes to refer to dimension. An image is a rank 3 tensor. 

## Create a unit matrix

```python
x = torch.ones(nrow, ncol)
```

## Replace with randome number in matrix

In pytorch, underscore means you don't want to report the random number but replace the matrix column.

```python
x[:, 0].uniform_(-1., 1)
```

## Do multiplication

```python
y = x@a + torch.rand(n)
```

## Batch Gradient Descent

One time update to use all the data point

```python
a = nn.Parameter(a)

def update():
    y_hat = x@a #requires two tensor but no grad requires
    loss = mse(y, y_hat)
    if t % 10 == 0: print(loss)
    loss.backward() #then it will automatically store the gradient to object a.
    with torch.no_grad():
        a.sub_(lr * a.grad) #inplace subtraction
        a.grad.zero_() #inplace initrialize the gradient to 0 for next batch.
```

`no_grad` will make all the operations in the block have no gradients.

In pytorch, you can't do inplacement changing of w1 and w2, which are two variables with require\_grad = True. I think that avoiding the inplacement changing of w1 and w2 is because it will cause error in back propagation calculation. Since inplacement change will totally change w1 and w2.

However, if you use this no\_grad\(\), you can control the new w1 and new w2 have no gradients since they are generated by operations, which means you only change the value of w1 and w2, not gradient part, they still have previous defined variable gradient information and back propagation can continue.

## Mini-batch Gradient Descent

Basically, we take a mini batch of x to do the update.

```python
a = nn.Parameter(a)

def update():
    y_hat = x[rand_idx]@a #requires two tensor but no grad requires
    loss = mse(y[rand_indx], y_hat)
    if t % 10 == 0: print(loss)
    loss.backward() #then it will automatically store the gradient to object a.
    with torch.no_grad():
        a.sub_(lr * a.grad) #inplace subtraction
        a.grad.zero_() #inplace initrialize the gradient to 0 for next batch.
```

And stochastic gradient descent is just take one sample at a time.

